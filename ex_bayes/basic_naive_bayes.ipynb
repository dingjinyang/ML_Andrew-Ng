{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 朴素贝叶斯基础\n",
    "\n",
    "贝叶斯是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题。算法以自变量之间的独立（条件特征独立）性和连续变量的正态性假设为前提，就会导致算法精度在某种程度上受影响。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 贝叶斯理论"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 贝叶斯决策理论\n",
    "\n",
    "假设我们有数据集，由两类数据组成：\n",
    "\n",
    "- 用 $p_1(x, y)$ 表示数据点 $(x, y)$ 属于类别1的概率\n",
    "- 用 $p_2(x, y)$ 表示数据点 $(x, y)$ 属于类别2的概率\n",
    "\n",
    "则我们可以用一下规则来判断它的类别：\n",
    "\n",
    "- 如果 $p_1(x, y) > p_2(x, y)$，那么类别为 1\n",
    "- 如果 $p_1(x, y) < p_2(x, y)$，那么类别为 2\n",
    "\n",
    "也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 条件概率\n",
    "\n",
    "条件概率指在事件B发生的条件下，事件A发生的概率，用 $P (A \\mid B)$ 来表示。\n",
    "\n",
    "![](https://pic2.zhimg.com/9f9e13fbb2f473496f04f565f1e1b2af_r.jpg?source=1940ef5c)\n",
    "\n",
    "由图可以看出，在事件B发生的条件下事件A发生的概率为 \n",
    "$$P(A \\mid B) = \\frac{P(AB)}{P(B)}$$ \n",
    "因此\n",
    "$$P(AB) = P(A \\mid B)P(B)$$\n",
    "同理\n",
    "$$P(AB) = P(B \\mid A)P(A)$$\n",
    "所以\n",
    "$$P(A \\mid B)P(B) = P(B \\mid A)P(A)$$\n",
    "即\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 全概率公式\n",
    "\n",
    "假定样本空间 S，是两个事件 A 与 A‘ 的和\n",
    "\n",
    "![](https://cuijiahua.com/wp-content/uploads/2017/11/ml_4_9.jpg)\n",
    "\n",
    "事件 B 可以划分为两个部分\n",
    "\n",
    "![](https://cuijiahua.com/wp-content/uploads/2017/11/ml_4_10.jpg)\n",
    "\n",
    "即\n",
    "$$P(B) = P(BA) + P(BA')$$\n",
    "由已知 $P(BA) = P(B \\mid A)P(A)$得，\n",
    "$$P(B) = P(B \\mid A)P(A) + P(B \\mid A')P(A')$$\n",
    "这就是全概率公式。\n",
    "\n",
    "代入条件概率公式可得\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A')P(A')}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 贝叶斯推断\n",
    "对条件概率公式变形，可得 $$P(A \\mid B) = P(A) \\frac{P(B \\mid A)}{P(B)}$$\n",
    "\n",
    "**$P(A)$ 为“先验概率”**，即在事件B发生之前，对事件A概率的一个判断；\n",
    "\n",
    "**$P(A \\mid B)$ 为“后验概率”**，即在事件B发生之后，对事件A概率的重新评估；\n",
    "\n",
    "**$\\frac{P(B \\mid A)}{P(B)}$ 为“可能性函数”**，这是一个调整因子，使得预估概率更接近真实概率，\n",
    "\n",
    "故条件概率可以理解为 $$后验概率 = 先验概率 \\times 调整因子$$\n",
    "\n",
    "**贝叶斯推断的含义：先预估一个“先验概率”，然后加入实验结果，看这个实验到底是增强还是削弱了“先验概率”，由此得到更接近事实的“后验概率”**。在这里，如果“可能性函数” $\\frac{P(B \\mid A)}{P(B)} > 1$，意味着“先验概率”被增强，事件A发生的可能性变大；如果 $\\frac{P(B \\mid A)}{P(B)} = 1$，意味着事件B无助于判断事件A的可能性；如果 $\\frac{P(B \\mid A)}{P(B)} < 1$，意味着“先验概率”被削弱，事件A的可能性变小。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "举例：\n",
    "\n",
    "![](https://cuijiahua.com/wp-content/uploads/2017/11/ml_4_16.jpg)\n",
    "\n",
    "两个一模一样的碗，一号碗有30颗水果糖和10颗巧克力糖，二号碗有水果糖和巧克力糖各20颗。现在随机选择一个碗，从中摸出一颗糖，发现是水果糖。请问这颗水果糖来自一号碗的概率有多大？\n",
    "\n",
    "设 $A$ 表示一号碗，$B$ 表示二号碗。由于两个碗是一样的，所以 $P(A) = P(B)$，即在取水果之前，两个碗被选中的概率相同，即 $P(A) = 0.5$，这个概率叫做“先验概率”，即没有做实验之前，来自一号碗的概率是 0.5。\n",
    "\n",
    "设 $X$ 表示水果糖，问题为在已知 X 的情况下，来自一号碗的概率有多大，即求 $P(A \\mid X)$，这个概率叫做“后验概率”，即在 X 事件发生之后，对 $P(A)$ 的修正。\n",
    "\n",
    "根据条件概率公式，可得 $$P(A \\mid X) = P(A) \\frac{P(X \\mid A)}{P(X)}$$\n",
    "其中 $P(X \\mid A) = \\frac{30}{30 + 10} = 0.75,\\\\ P(X \\mid B) = \\frac{20}{20 + 20} = 0.5,\\\\ P(X) = P(X \\mid A)P(A) + P(X \\mid B)P(B) = 0.75 \\times 0.5 + 0.5 \\times 0.5 = 0.625$ \n",
    "代入得，$P(A \\mid X) = 0.5 \\times \\frac{0.75}{0.625} = 0.6$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 朴素贝叶斯推断\n",
    "\n",
    "朴素贝叶斯对条件概率分布做了独立性的假设。假设下面公式有 n 个特征：\n",
    "$$P(A \\mid X) = \\frac{P(X \\mid A)P(A)}{P(X)} = \\frac{P(x_1, x_2, x_3, \\dots, x_n \\mid A)P(A)}{P(x_1, x_2, x_3, \\dots, x_n)}$$\n",
    "\n",
    "由于 $P(x_1, x_2, x_3, \\dots, x_n)$ 对于所有的类别都是相同的，可以省略，问题就变成了求\n",
    "$P(x_1, x_2, x_3, \\dots, x_n \\mid A)P(A)$，\n",
    "\n",
    "假设所有特征都是相互独立的，故\n",
    "$$P(x_1, x_2, x_3, \\dots, x_n \\mid A)P(A) = P(x_1 \\mid A)P(x_2 \\mid A)P(x_3 \\mid A) \\dots P(x_n \\mid A)P(A)$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "举例：\n",
    "\n",
    "某个医院早上收了六个门诊病人，如下表：\n",
    "\n",
    "```\n",
    "症状　　    职业　　　    疾病\n",
    "        \n",
    "打喷嚏　    护士　　　    感冒\n",
    "打喷嚏　    农夫　　　    过敏\n",
    "头痛　　    建筑工人　    脑震荡\n",
    "头痛　　    建筑工人　    感冒\n",
    "打喷嚏　    教师　　　    感冒\n",
    "头痛　　    教师　　　    脑震荡\n",
    "```\n",
    "\n",
    "现在又来了第七个病人，是一个打喷嚏的建筑工人，请问他患上感冒的概率有多大？\n",
    "\n",
    "根据贝叶斯定理：\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}$$\n",
    "\n",
    "可得：\n",
    "$$P(感冒 \\mid 打喷嚏, 建筑工人) = \\frac{P(打喷嚏, 建筑工人 \\mid 感冒)P(感冒)}{P(打喷嚏, 建筑工人)}$$\n",
    "\n",
    "由朴素贝叶斯条件独立性的假设可知，“打喷嚏“和”建筑工人“两个特征是独立的，因此上面的等式变为\n",
    "$$P(感冒 \\mid 打喷嚏, 建筑工人) = \\frac{P(打喷嚏 \\mid 感冒)P(建筑工人 \\mid 感冒)P(感冒)}{P(打喷嚏)P(建筑工人)}$$\n",
    "\n",
    "可以计算：\n",
    "$$P(感冒 \\mid 打喷嚏, 建筑工人) = \\frac{0.66 \\times 0.33 \\times 0.5}{0.5 \\times 0.33} = 0.66$$\n",
    "\n",
    "因此，这个打喷嚏的建筑工人，有66%的概率是得了感冒。同理，可以计算这个病人患上过敏或脑震荡的概率。比较这几个概率，就可以知道他最可能得什么病。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 动手实战 - 文本分类\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 初始化数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "            ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "            ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "            ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "            ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "            ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "y = [0, 1, 0, 1, 0, 1]  "
   ]
  },
  {
   "source": [
    "### 构建词向量"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['help', 'ate', 'I', 'take', 'to', 'not', 'dog', 'him', 'park', 'licks', 'problems', 'how', 'please', 'garbage', 'my', 'cute', 'stop', 'dalmation', 'buying', 'is', 'has', 'quit', 'steak', 'worthless', 'food', 'maybe', 'so', 'mr', 'stupid', 'flea', 'love', 'posting']\n[[1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n [0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0]\n [0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1]\n [0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建所有词的词向量\n",
    "words_list = list({w for ws in data_set for w in ws})  \n",
    "print(words_list)\n",
    "X = list()\n",
    "for ws in data_set:\n",
    "    tmp = [0] * len(words_list)\n",
    "    for i in range(len(words_list)):\n",
    "        if words_list[i] in ws:\n",
    "            tmp[i] = 1\n",
    "    X.append(tmp)\n",
    "X, y= np.array(X), np.array(y)\n",
    "print(X)"
   ]
  },
  {
   "source": [
    "### 概率计算\n",
    "\n",
    "现在 已经知道一个词是否出现在一篇文档中，也知道该文档所属的类别。从词向量计算概率，公式为 $$P(c_i \\mid w) = \\frac{P(w \\mid c_i)P(c_i)}{p(c_i)} , (w 为词向量)$$\n",
    "\n",
    "对每个类计算该值，并比较概率值的大小。\n",
    "\n",
    "由 $P(c_i) = \\frac{类别i(侮辱性留言或非侮辱性留言)中的文档数}{总的文档数}$\n",
    "\n",
    "可得 $P(c_0) = P(非侮辱性) = 0.5$; $P(c_1) = P(侮辱性) = 0.5$。\n",
    "\n",
    "而 $P(w \\mid c_i)$ 根据独立性假设 ，可使用 $P(w_0 \\mid c_i)P(w_1 \\mid c_i)P(w_2 \\mid c_i) \\cdots P(w_n \\mid c_i)$ 来计算。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train_naive_bayes0(X, y):\n",
    "    m, n = X.shape[0], X.shape[1]\n",
    "    print(m, n)\n",
    "    p_abusive = np.sum(y) / float(m)\n",
    "    print(p_abusive)\n",
    "    p0, p1 = np.zeros(n), np.zeros(n)\n",
    "    p0_denom, p1_denom = 0.0, 0.0\n",
    "    for i in range(m):\n",
    "        if y[i] == 1:\n",
    "            p1 += X[i]\n",
    "            p1_denom += sum(X[i])\n",
    "        else:\n",
    "            p0 += X[i]\n",
    "            p0_denom += sum(X[i])\n",
    "    return p0 / p0_denom, p1 / p1_denom, p_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6 32\n0.5\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.04166667, 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "        0.        , 0.04166667, 0.08333333, 0.        , 0.04166667,\n",
       "        0.04166667, 0.04166667, 0.04166667, 0.        , 0.125     ,\n",
       "        0.04166667, 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "        0.04166667, 0.        , 0.04166667, 0.        , 0.        ,\n",
       "        0.        , 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "        0.04166667, 0.        ]),\n",
       " array([0.        , 0.        , 0.        , 0.05263158, 0.05263158,\n",
       "        0.05263158, 0.10526316, 0.05263158, 0.05263158, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05263158, 0.        ,\n",
       "        0.        , 0.05263158, 0.        , 0.05263158, 0.        ,\n",
       "        0.        , 0.05263158, 0.        , 0.10526316, 0.05263158,\n",
       "        0.05263158, 0.        , 0.        , 0.15789474, 0.        ,\n",
       "        0.        , 0.05263158]),\n",
       " 0.5)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_naive_bayes0(X, y)"
   ]
  },
  {
   "source": [
    "函数 `train_naive_baye` 返回3个值， 分别为每个单词属于类别0的概率，每个单词属于类别1的概率和每个类别的概率"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 根据情况修改算法\n",
    "\n",
    "利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 $P(w_0|1)P(w_1|1)P(w_2|1)$。如果其中一个概率值为0，那么最后的乘积也为0。为降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。\n",
    "```python\n",
    "p0, p1 = np.ones(n), np.ones(n)\n",
    "p0_denom, p1_denom = 2.0, 2.0\n",
    "```\n",
    "\n",
    "另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积 $P(w_0|c_i)P(w_1|c_i)P(w_2|c_i) \\cdots P(w_n|c_i)$ 时，由于大部分因子都非常小，所以程序会下溢出或者 得到不正确的答案。一种解决办法是对乘积取自然对数。在代数中有 $ln(a \\times b) = ln(a)+ln(b)$，于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(X, y):\n",
    "    \"\"\"\n",
    "    训练贝叶斯模型\n",
    "    \"\"\"\n",
    "    m, n = X.shape[0], X.shape[1]\n",
    "    p_abusive = np.sum(y) / float(m)\n",
    "    p0, p1 = np.ones(n), np.ones(n)\n",
    "    p0_denom, p1_denom = 2.0, 2.0\n",
    "    for i in range(m):\n",
    "        if y[i] == 1:\n",
    "            p1 += X[i]\n",
    "            p1_denom += sum(X[i])\n",
    "        else:\n",
    "            p0 += X[i]\n",
    "            p0_denom += sum(X[i])\n",
    "    return np.log(p0 / p0_denom), np.log(p1 / p1_denom), p_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2vec(words):\n",
    "    vec = [0] * len(words_list)\n",
    "    for i in range(len(words_list)):\n",
    "        if words_list[i] in words:\n",
    "            vec[i] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_vec, p0_v, p1_v, pc):\n",
    "    p0 = np.sum(test_vec * p0_v) + np.log(1 - pc)\n",
    "    p1 = np.sum(test_vec * p1_v) + np.log(pc)\n",
    "    return 0 if p0 > p1 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['love', 'my', 'dalmation'] classify as 0\n['stupid', 'garbage'] classify as 1\n"
     ]
    }
   ],
   "source": [
    "def testing():\n",
    "    p0_v, p1_v, pc = train_naive_bayes(X, y)\n",
    "    test_words = ['love', 'my', 'dalmation']\n",
    "    test_vec = np.array(words2vec(test_words))\n",
    "    print(f'{test_words} classify as {classify(test_vec, p0_v, p1_v, pc)}')\n",
    "    test_words = ['stupid', 'garbage']\n",
    "    test_vec = np.array(words2vec(test_words))\n",
    "    print(f'{test_words} classify as {classify(test_vec, p0_v, p1_v, pc)}')\n",
    "testing()"
   ]
  }
 ]
}